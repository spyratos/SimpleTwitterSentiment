{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import preprocessing\n",
    "import features\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw2013tr=pd.read_csv('twitter-2013train-A.tsv',sep='\\t',header=None)\n",
    "tw2015tr=pd.read_csv('twitter-2015train-A.tsv',sep='\\t',header=None)\n",
    "tw2016tr=pd.read_csv('twitter-2016train-A.tsv',sep='\\t',header=None)\n",
    "twAtest=pd.read_csv('twitter-2016test-A.tsv',sep='\\t',header=None)\n",
    "\n",
    "twdev=pd.read_csv('twitter-2016dev-A.tsv',sep='\\t',header=None)\n",
    "twdevtest=pd.read_csv('twitter-2016devtest-A.tsv',sep='\\t',header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Test\n",
    "Datasets concatenation and column name addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train=tw2013tr.append(tw2015tr).append(tw2016tr)#.append(twdev)\n",
    "twtest=twAtest.append(twdevtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train.columns=['id','label','tweet']\n",
    "twtest.columns=['id','label','tweet']\n",
    "\n",
    "twdevtest.columns=['id','label','tweet']\n",
    "twdev.columns=['id','label','tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of Unavailable tweets\n",
    "and reindexing of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train = all_train[all_train.tweet != \"Not Available\"]\n",
    "twtest = twtest[twtest.tweet != \"Not Available\"]\n",
    "\n",
    "twdevtest = twdevtest[twdevtest.tweet != \"Not Available\"]\n",
    "twdev = twdev[twdev.tweet != \"Not Available\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train = all_train.reset_index(drop=True)\n",
    "twtest = twtest.reset_index(drop=True)\n",
    "\n",
    "twdevtest = twdevtest.reset_index(drop=True)\n",
    "twdev = twdev.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Html entities decode to pure text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "h = HTMLParser()\n",
    "all_train['tweet']=all_train.tweet.apply(h.unescape)\n",
    "twtest['tweet']=twtest.tweet.apply(h.unescape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words removal (deceided not do apply it to the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "\n",
    "#cachedStopWords = stopwords.words(\"english\")\n",
    "#def removestopwords(text):\n",
    "#    text = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "#    return text\n",
    "\n",
    "#for i in range(all_train.tweet.count()):\n",
    "#    all_train.iloc[i,all_train.columns.get_loc('tweet')] = removestopwords(all_train.tweet.iloc[i])\n",
    "\n",
    "#for i in range(twtest.tweet.count()):\n",
    "#    twtest.iloc[i,twtest.columns.get_loc('tweet')] = removestopwords(twtest.tweet.iloc[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing steps for tweets \n",
    "and some feature extraction for our other method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(all_train.tweet.count()):\n",
    "    all_train.iloc[i,all_train.columns.get_loc('tweet')] = preprocessing.processAll(all_train.tweet.iloc[i])\n",
    "\n",
    "all_train['features']='0'\n",
    "temp_list=[]\n",
    "for i in range(all_train.tweet.count()):\n",
    "    temp_list.append(features.tweet_features.make_tweet_nparr(all_train.tweet.iloc[i]))\n",
    "all_train['features']=temp_list\n",
    "\n",
    "\n",
    "\n",
    "for i in range(twtest.tweet.count()):\n",
    "    twtest.iloc[i,twtest.columns.get_loc('tweet')] = preprocessing.processAll(twtest.tweet.iloc[i])\n",
    "\n",
    "twtest['features']='0'\n",
    "temp_list=[]\n",
    "for i in range(twtest.tweet.count()):\n",
    "    temp_list.append(features.tweet_features.make_tweet_nparr(twtest.tweet.iloc[i]))\n",
    "twtest['features']=temp_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(data=all_train.features.iloc[0].reshape(-1, len(all_train.features.iloc[0])))\n",
    "for i in range(1,all_train.tweet.count()):\n",
    "    x=x.append(pd.DataFrame(all_train.features.iloc[i].reshape(-1, len(all_train.features.iloc[i]))))\n",
    "y=pd.DataFrame(data=twtest.features.iloc[0].reshape(-1, len(twtest.features.iloc[0])))\n",
    "for i in range(1,twtest.tweet.count()):\n",
    "    y=y.append(pd.DataFrame(twtest.features.iloc[i].reshape(-1, len(twtest.features.iloc[i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Multinomial Naive Bayes Classifier \n",
    "with 1,2,3 ngrams, TfIdf and text length as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
    "pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('text', Pipeline([\n",
    "                ('vectorizer', CountVectorizer(min_df=0.002,ngram_range=(1, 3))),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "            ])),\n",
    "            ('length', Pipeline([\n",
    "                ('count', FunctionTransformer(get_text_length, validate=False)),\n",
    "            ]))\n",
    "        ])),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fit and predection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Method\n",
    "Multinomial Naive Bayes with 1,2,3 ngrams, TfIdf and text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59542206471852666"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(all_train.tweet, all_train.label)\n",
    "predicted = pipeline.predict(twtest.tweet)\n",
    "np.mean(predicted == twtest.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.57493103948091095, 0.59542206471852666, 0.55886148929366508, None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "precision_recall_fscore_support(twtest.label, predicted, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Method\n",
    "Multinomial Naive Bayes with vector of complex features (explained in the readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57032475454598264"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=MultinomialNB()\n",
    "clf = clf.fit(x, all_train.label)\n",
    "\n",
    "predicted = clf.predict(y)\n",
    "np.mean(predicted == twtest.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.61017881240713601, 0.57032475454598264, 0.50908300927969385, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(twtest.label, predicted, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd Method\n",
    "Use of the pretrained tool TextBlob (with a mapping of its polarities to our labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54999128565619004"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "polarities=[]\n",
    "for i in range(twtest.tweet.count()):\n",
    "    testimonial = TextBlob(twtest.tweet.iloc[i])\n",
    "    polarities.append(testimonial.sentiment.polarity)\n",
    "\n",
    "    marika=[]\n",
    "for i in range(twtest.tweet.count()):\n",
    "    if polarities[i]>0.3:\n",
    "        marika.append('positive')\n",
    "    elif polarities[i]< -0.3:\n",
    "        marika.append('negative')\n",
    "    else:\n",
    "        marika.append('neutral')\n",
    "        \n",
    "np.mean(marika == twtest.label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5481127812272778, 0.54999128565619004, 0.51443250960099485, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(twtest.label, marika, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4th Method\n",
    "Use of the pretrained tool Pattern (with a mapping of its polarities to our labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54923604252599778"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pattern.en  import sentiment as sentiment_en\n",
    "    \n",
    "polarities=[]\n",
    "for i in range(twtest.tweet.count()):\n",
    "    testimonial = sentiment_en(twtest.tweet.iloc[i])\n",
    "    polarities.append(testimonial[0])\n",
    "marika=[]\n",
    "for i in range(twtest.tweet.count()):\n",
    "    if polarities[i]>0.3:\n",
    "        marika.append('positive')\n",
    "    elif polarities[i]< -0.3:\n",
    "        marika.append('negative')\n",
    "    else:\n",
    "        marika.append('neutral')\n",
    "        \n",
    "np.mean(marika == twtest.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.54619414967032565, 0.54923604252599778, 0.51340252319815693, None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(twtest.label, marika, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
